\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{srgan_2016}
\citation{esrgan_2018,srgan_2016,weighted_srgan_2021}
\citation{Hitchhiker_guide_super_res_2023,sr_ill_posed_2021}
\citation{sr_technical_overview_2003}
\citation{video_super_resolution_survey_2020}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{IEEE_Journal.ist}
\@glsorder{word}
\pgfsyspdfmark {pgfid1}{3876290}{22266606}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Example of the super-resolved frame generated using the SRGAN model (sequence 00001/0628 from Vimeo90K dataset)}}{1}{figure.1}\protected@file@percent }
\newlabel{fig:example_sr_conclusion}{{1}{1}{Example of the super-resolved frame generated using the SRGAN model (sequence 00001/0628 from Vimeo90K dataset)}{figure.1}{}}
\citation{srgan_2016}
\citation{gans_overview_2018}
\citation{gans_overview_2018}
\citation{gans_overview_2018}
\citation{gans_overview_2018}
\citation{video_super_resolution_survey_2020}
\citation{video_super_resolution_survey_2020}
\citation{video_super_resolution_survey_2020}
\citation{video_super_resolution_survey_2020}
\citation{frvsr_2018}
\citation{video_super_resolution_survey_2020}
\citation{video_super_resolution_survey_2020}
\@writefile{toc}{\contentsline {section}{\numberline {II}Literature Review}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Generative Adversarial Networks}{2}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A classification scheme for the current state-of-the-art video super-resolution methods \cite  {video_super_resolution_survey_2020}}}{2}{figure.2}\protected@file@percent }
\newlabel{fig:vsr_methods}{{2}{2}{A classification scheme for the current state-of-the-art video super-resolution methods \cite {video_super_resolution_survey_2020}}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Video Super-Resolution Overview}{2}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-C}}Frame-Reccurent Video Super-Resolution}{2}{subsection.2.3}\protected@file@percent }
\citation{tecogan_2018}
\citation{video_super_resolution_survey_2020}
\citation{GANs_perc_loss_vsr_2018}
\citation{Yan_frame_vsr_2019}
\citation{iSeeBetter_2020}
\citation{iSeeBetter_2020}
\citation{iSeeBetter_2020}
\citation{fast_video_super_reso_ann_2012}
\citation{universal_image_quality_index_2002}
\citation{iSeeBetter_2020}
\citation{iSeeBetter_2020}
\citation{vgg_very_deep_cnn_2014}
\citation{sr_with_deep_conv_sufficient_stats_2015,texture_synth_cnn_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The network architecture of FRVSR \cite  {video_super_resolution_survey_2020}}}{3}{figure.3}\protected@file@percent }
\newlabel{fig:frvsr_arch}{{3}{3}{The network architecture of FRVSR \cite {video_super_resolution_survey_2020}}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-D}}TecoGAN}{3}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The network architecture of TecoGAN \cite  {video_super_resolution_survey_2020}}}{3}{figure.4}\protected@file@percent }
\newlabel{fig:tecogan}{{4}{3}{The network architecture of TecoGAN \cite {video_super_resolution_survey_2020}}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-E}}VSRResFeatGAN}{3}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-F}}iSeeBetter}{3}{subsection.2.6}\protected@file@percent }
\newlabel{sec:loss_function}{{\mbox  {II-G}}{3}{Loss function}{subsection.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-G}}Loss function}{3}{subsection.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The \acrshort {mse}, perceptual, adversarial and \acrshort {tv} loss components \cite  {iSeeBetter_2020}.}}{3}{figure.5}\protected@file@percent }
\newlabel{fig:loss}{{5}{3}{The \acrshort {mse}, perceptual, adversarial and \acrshort {tv} loss components \cite {iSeeBetter_2020}}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-G}1}Mean-Squared Error Loss}{3}{subsubsection.2.7.1}\protected@file@percent }
\newlabel{eq:mse_loss}{{1}{3}{Mean-Squared Error Loss}{equation.2.1}{}}
\citation{iSeeBetter_2020}
\citation{goodfellow2014generative}
\citation{image_upsampling_total_variation_regularization_2005}
\citation{deep_learning_image_sr_2020}
\citation{nonlinear_total_variation_noise_removal_1992}
\citation{srgan_2016}
\citation{iSeeBetter_2020}
\citation{iSeeBetter_milestone}
\citation{SRGAN_with_tv_loss_face_2020}
\citation{Perceptual_Losses_for_Real_Time_Style_Transfer_and_Super_Resolution_2016}
\citation{Perceptual_Losses_for_Real_Time_Style_Transfer_and_Super_Resolution_2016}
\citation{srgan_2016}
\citation{image_upsampling_total_variation_regularization_2005}
\citation{Perceptual_Losses_for_Real_Time_Style_Transfer_and_Super_Resolution_2016}
\citation{SRGAN_with_tv_loss_face_2020}
\citation{srgan_2016}
\citation{Perceptual_Losses_for_Real_Time_Style_Transfer_and_Super_Resolution_2016}
\citation{iSeeBetter_2020}
\citation{HandsOn_GANs_2019}
\citation{iSeeBetter_milestone}
\citation{single_vsr_gan_pseudo_inverse_2019}
\citation{vimeo90k_2019}
\citation{deep_learning_image_sr_2020}
\newlabel{sec:perceptual_loss}{{\mbox  {II-G}2}{4}{Perceptual Loss}{subsubsection.2.7.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-G}2}Perceptual Loss }{4}{subsubsection.2.7.2}\protected@file@percent }
\newlabel{sec:adversarial_loss}{{\mbox  {II-G}3}{4}{Adversarial Loss}{subsubsection.2.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-G}3}Adversarial Loss}{4}{subsubsection.2.7.3}\protected@file@percent }
\newlabel{eq:adversarial_loss}{{2}{4}{Adversarial Loss}{equation.2.2}{}}
\newlabel{sec:total_variation_loss}{{\mbox  {II-G}4}{4}{Total-Variation Loss}{subsubsection.2.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-G}4}Total-Variation Loss}{4}{subsubsection.2.7.4}\protected@file@percent }
\newlabel{eq:tv_loss}{{\mbox  {II-G}4}{4}{Total-Variation Loss}{subsubsection.2.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {II-G}5}Loss formulation}{4}{subsubsection.2.7.5}\protected@file@percent }
\newlabel{eq:total_Gloss}{{3}{4}{Loss formulation}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-H}}TV loss weight}{4}{subsection.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Methodology}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Dataset}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Detailed Methodology}{4}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Evaluation Metrics}{4}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Sample workflow image. Make it in Powerpoint with svg images and save as pdf. Sanity check: Zoom in and pixels should not break.}}{5}{figure.6}\protected@file@percent }
\newlabel{Fig:Figure1}{{6}{5}{Sample workflow image. Make it in Powerpoint with svg images and save as pdf. Sanity check: Zoom in and pixels should not break}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Sample workflow image. Make it in Powerpoint with svg images and save as pdf. Sanity check: Zoom in and pixels should not break.}}{5}{figure.7}\protected@file@percent }
\newlabel{Fig:Figure3b}{{7}{5}{Sample workflow image. Make it in Powerpoint with svg images and save as pdf. Sanity check: Zoom in and pixels should not break}{figure.7}{}}
\citation{video_super_resolution_survey_2020}
\citation{deep_learning_image_sr_2020}
\citation{ssim_2004}
\citation{deep_learning_image_sr_2020}
\citation{deep_learning_image_sr_2020}
\citation{bayesian_vsr_2014}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Literature review table showing the contributions of various authors for quantization of networks.}}{6}{table.1}\protected@file@percent }
\newlabel{tab:tv_loss_summary}{{I}{6}{Literature review table showing the contributions of various authors for quantization of networks}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Datasets summary.}}{6}{table.2}\protected@file@percent }
\newlabel{tab:datasets}{{II}{6}{Datasets summary}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}1}Mean Squared Error}{6}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{eq:mse_metric}{{4}{6}{Mean Squared Error}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}2}Peak Signal to Noise Ratio}{6}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{eq:psnr_metric}{{5}{6}{Peak Signal to Noise Ratio}{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {III-C}3}Structural Similarity Index Metric}{6}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{eq:comp_luminosity}{{6}{6}{Structural Similarity Index Metric}{equation.3.6}{}}
\newlabel{eq:comp_contrast}{{7}{6}{Structural Similarity Index Metric}{equation.3.7}{}}
\newlabel{eq:ssim}{{8}{6}{Structural Similarity Index Metric}{equation.3.8}{}}
\newlabel{eq:structure_comp_f}{{9}{6}{Structural Similarity Index Metric}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Experimental settings}{6}{subsection.3.4}\protected@file@percent }
\bibstyle{IEEEtran}
\bibdata{Bibliography}
\bibcite{srgan_2016}{1}
\bibcite{esrgan_2018}{2}
\bibcite{weighted_srgan_2021}{3}
\bibcite{Hitchhiker_guide_super_res_2023}{4}
\bibcite{sr_ill_posed_2021}{5}
\bibcite{sr_technical_overview_2003}{6}
\bibcite{video_super_resolution_survey_2020}{7}
\bibcite{gans_overview_2018}{8}
\bibcite{frvsr_2018}{9}
\bibcite{tecogan_2018}{10}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Sample network architecture image. Make it in Powerpoint with svg images and save as pdf. Sanity check: Zoom in and pixels should not break.}}{7}{figure.8}\protected@file@percent }
\newlabel{Fig:Figure2}{{8}{7}{Sample network architecture image. Make it in Powerpoint with svg images and save as pdf. Sanity check: Zoom in and pixels should not break}{figure.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Configuration table showing the network configuration of FCN used in this study. The table shows the various configuration settings used for FCN8.}}{7}{table.3}\protected@file@percent }
\newlabel{tab:FCNConfiguration}{{III}{7}{Configuration table showing the network configuration of FCN used in this study. The table shows the various configuration settings used for FCN8}{table.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{7}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{7}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{7}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{7}{section*.1}\protected@file@percent }
\bibcite{GANs_perc_loss_vsr_2018}{11}
\bibcite{Yan_frame_vsr_2019}{12}
\bibcite{iSeeBetter_2020}{13}
\bibcite{fast_video_super_reso_ann_2012}{14}
\bibcite{universal_image_quality_index_2002}{15}
\bibcite{vgg_very_deep_cnn_2014}{16}
\bibcite{sr_with_deep_conv_sufficient_stats_2015}{17}
\bibcite{texture_synth_cnn_2015}{18}
\bibcite{goodfellow2014generative}{19}
\bibcite{image_upsampling_total_variation_regularization_2005}{20}
\bibcite{deep_learning_image_sr_2020}{21}
\bibcite{nonlinear_total_variation_noise_removal_1992}{22}
\bibcite{iSeeBetter_milestone}{23}
\bibcite{SRGAN_with_tv_loss_face_2020}{24}
\bibcite{Perceptual_Losses_for_Real_Time_Style_Transfer_and_Super_Resolution_2016}{25}
\bibcite{HandsOn_GANs_2019}{26}
\bibcite{single_vsr_gan_pseudo_inverse_2019}{27}
\bibcite{vimeo90k_2019}{28}
\bibcite{ssim_2004}{29}
\bibcite{bayesian_vsr_2014}{30}
\bibcite{b2}{31}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Performance comparison table showing the performance of various quantization techniques applied on various networks. The results are directly taken from the papers after applying quantization on the mentioned datasets.}}{8}{table.4}\protected@file@percent }
\newlabel{tab:LiteratureComparison}{{IV}{8}{Performance comparison table showing the performance of various quantization techniques applied on various networks. The results are directly taken from the papers after applying quantization on the mentioned datasets}{table.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Sample Figure comparing the three quantization techniques Fixed Point (FP), Lloyd's quantizer (LQ) and $L_2$ error minimization ($L_2$) on the three performance metrics divided into encoder and decoder layers. Mean IoU is shown for the three techniques in Panel A), pixel accuracy in Panel B), and mean accuracy in Panel C) respectively. Note that FP is consistently worse than both LQ and $L_2$, while $L_2$ and LQ are of comparable accuracy. Also, FP is most sensitive to number of bits in all metrics while $L_2$ and LQ are relatively insensitive.}}{9}{figure.9}\protected@file@percent }
\newlabel{Fig:Figure4}{{9}{9}{Sample Figure comparing the three quantization techniques Fixed Point (FP), Lloyd's quantizer (LQ) and $L_2$ error minimization ($L_2$) on the three performance metrics divided into encoder and decoder layers. Mean IoU is shown for the three techniques in Panel A), pixel accuracy in Panel B), and mean accuracy in Panel C) respectively. Note that FP is consistently worse than both LQ and $L_2$, while $L_2$ and LQ are of comparable accuracy. Also, FP is most sensitive to number of bits in all metrics while $L_2$ and LQ are relatively insensitive}{figure.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Other headings and reference material}{9}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VII-A}}Ease of Use}{9}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VII-A}1}Maintaining the Integrity of the Specifications}{9}{subsubsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {VII-B}}Prepare Your Report Before Styling}{9}{subsection.7.2}\protected@file@percent }
\newlabel{AA}{{\mbox  {VII-B}1}{9}{Abbreviations and Acronyms}{subsubsection.7.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VII-B}1}Abbreviations and Acronyms}{9}{subsubsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VII-B}2}Units}{9}{subsubsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VII-B}3}Equations}{9}{subsubsection.7.2.3}\protected@file@percent }
\newlabel{eq}{{10}{9}{Equations}{equation.7.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VII-B}4}\LaTeX  -Specific Advice}{9}{subsubsection.7.2.4}\protected@file@percent }
\citation{bayesian_vsr_2014}
\newlabel{SCM}{{\mbox  {VII-B}5}{10}{Some Common Mistakes}{subsubsection.7.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VII-B}5}Some Common Mistakes}{10}{subsubsection.7.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VII-B}6}Authors and Affiliations}{10}{subsubsection.7.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VII-B}7}Identify the Headings}{10}{subsubsection.7.2.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {\mbox  {VII-B}8}Figures and Tables}{10}{subsubsection.7.2.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\numberline {\mbox  {VII-B}8a}Positioning Figures and Tables}{10}{paragraph.7.2.8.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Table Type Styles}}{10}{table.5}\protected@file@percent }
\newlabel{tab1}{{V}{10}{Table Type Styles}{table.5}{}}
\citation{bayesian_vsr_2014}
\citation{b2}
\citation{bayesian_vsr_2014}
\citation{bayesian_vsr_2014}
\citation{bayesian_vsr_2014}
\citation{bayesian_vsr_2014}
\citation{bayesian_vsr_2014}
\citation{bayesian_vsr_2014}
\citation{bayesian_vsr_2014}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Example of a figure caption.}}{11}{figure.10}\protected@file@percent }
\newlabel{fig}{{10}{11}{Example of a figure caption}{figure.10}{}}
\gdef \@abspage@last{11}
